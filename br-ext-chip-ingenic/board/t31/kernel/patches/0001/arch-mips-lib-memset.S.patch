
--- a/arch/mips/lib/memset.S
+++ b/arch/mips/lib/memset.S
@@ -40,6 +40,12 @@
 	PTR	9b, handler;				\
 	.previous
 
+#define EXE(insn,handler)                               \
+9:      .word   insn;                                   \
+	.section __ex_table,"a"; 			\
+	PTR	9b, handler; 				\
+	.previous
+
 	.macro	f_fill64 dst, offset, val, fixup
 	EX(LONG_S, \val, (\offset +  0 * STORSIZE)(\dst), \fixup)
 	EX(LONG_S, \val, (\offset +  1 * STORSIZE)(\dst), \fixup)
@@ -63,6 +69,26 @@
 #endif
 	.endm
 
+	.macro  f_fill64eva dst, offset, val, fixup
+	.set        eva
+	EX(swe, \val, (\offset +  0 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset +  1 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset +  2 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset +  3 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset +  4 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset +  5 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset +  6 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset +  7 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset +  8 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset +  9 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset + 10 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset + 11 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset + 12 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset + 13 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset + 14 * STORSIZE)(\dst), \fixup)
+	EX(swe, \val, (\offset + 15 * STORSIZE)(\dst), \fixup)
+	.endm
+
 /*
  * memset(void *s, int c, size_t n)
  *
@@ -202,3 +228,142 @@ FEXPORT(__bzero)
 .Llast_fixup:
 	jr		ra
 	 andi		v1, a2, STORMASK
+
+#ifdef CONFIG_EVA
+/*  ++++++++  */
+/*  EVA stuff */
+/*  ++++++++  */
+
+	.set            eva
+
+#undef  LONG_S_L
+#undef  LONG_S_R
+
+#define LONG_S_L swle
+#define LONG_S_R swre
+
+LEAF(__bzero_user)
+	sltiu		t0, a2, STORSIZE	/* very small region? */
+	bnez            t0, .LEsmall_memset
+	 andi		t0, a0, STORMASK	/* aligned? */
+
+#ifdef CONFIG_CPU_MICROMIPS
+	move		t8, a1
+	move		t9, a1
+#endif
+#ifndef CONFIG_CPU_DADDI_WORKAROUNDS
+	beqz		t0, 1f
+	 PTR_SUBU	t0, STORSIZE		/* alignment in bytes */
+#else
+	.set		noat
+	li		AT, STORSIZE
+	beqz		t0, 1f
+	 PTR_SUBU	t0, AT			/* alignment in bytes */
+	.set		at
+#endif
+
+	R10KCBARRIER(0(ra))
+#ifdef __MIPSEB__
+	EX(LONG_S_L, a1, (a0), .LEfirst_fixup)  /* make word/dword aligned */
+#endif
+#ifdef __MIPSEL__
+	EX(LONG_S_R, a1, (a0), .LEfirst_fixup)  /* make word/dword aligned */
+#endif
+	PTR_SUBU	a0, t0			/* long align ptr */
+	PTR_ADDU	a2, t0			/* correct size */
+
+1:	ori		t1, a2, 0x3f		/* # of full blocks */
+	xori		t1, 0x3f
+	beqz            t1, .LEmemset_partial    /* no block to fill */
+	 andi		t0, a2, 0x40-STORSIZE
+
+	PTR_ADDU	t1, a0			/* end address */
+	.set		reorder
+1:	PTR_ADDIU	a0, 64
+	R10KCBARRIER(0(ra))
+	f_fill64eva a0, -64, a1, .LEfwd_fixup
+	bne		t1, a0, 1b
+	.set		noreorder
+
+.LEmemset_partial:
+	R10KCBARRIER(0(ra))
+	PTR_LA		t1, 2f			/* where to start */
+#ifdef CONFIG_CPU_MICROMIPS
+	LONG_SRL	t7, t0, 1
+#if LONGSIZE == 4
+	PTR_SUBU	t1, t7
+#else
+	.set		noat
+	LONG_SRL	AT, t7, 1
+	PTR_SUBU	t1, AT
+	.set		at
+#endif
+#else
+#if LONGSIZE == 4
+	PTR_SUBU	t1, t0
+#else
+	.set		noat
+	LONG_SRL	AT, t0, 1
+	PTR_SUBU	t1, AT
+	.set		at
+#endif
+#endif
+	jr		t1
+	 PTR_ADDU	a0, t0			/* dest ptr */
+
+	.set		push
+	.set		noreorder
+	.set		nomacro
+	f_fill64eva a0, -64, a1, .LEpartial_fixup   /* ... but first do longs ... */
+2:	.set		pop
+	andi		a2, STORMASK		/* At most one long to go */
+
+	beqz		a2, 1f
+	 PTR_ADDU	a0, a2			/* What's left */
+	R10KCBARRIER(0(ra))
+#ifdef __MIPSEB__
+	EX(LONG_S_R, a1, -1(a0), .LElast_fixup)
+#endif
+#ifdef __MIPSEL__
+	EX(LONG_S_L, a1, -1(a0), .LElast_fixup)
+#endif
+1:	jr		ra
+	 move		a2, zero
+
+.LEsmall_memset:
+	beqz		a2, 2f
+	 PTR_ADDU	t1, a0, a2
+
+1:	PTR_ADDIU	a0, 1			/* fill bytewise */
+	R10KCBARRIER(0(ra))
+	bne		t1, a0, 1b
+	 sb		a1, -1(a0)
+
+2:	jr		ra			/* done */
+	 move		a2, zero
+
+.LEfirst_fixup:
+	jr	ra
+	 nop
+
+.LEfwd_fixup:
+	PTR_L		t0, TI_TASK($28)
+	andi		a2, 0x3f
+	LONG_L		t0, THREAD_BUADDR(t0)
+	LONG_ADDU	a2, t1
+	jr		ra
+	 LONG_SUBU	a2, t0
+
+.LEpartial_fixup:
+	PTR_L		t0, TI_TASK($28)
+	andi		a2, STORMASK
+	LONG_L		t0, THREAD_BUADDR(t0)
+	LONG_ADDU	a2, t1
+	jr		ra
+	 LONG_SUBU	a2, t0
+
+.LElast_fixup:
+	jr		ra
+	 andi		v1, a2, STORMASK
+	END(__bzero_user)
+#endif
